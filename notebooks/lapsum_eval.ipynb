{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76c8104eb70ccabf",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook is for evaluation of LaPSUM.\n",
    "\n",
    "It expects the data to be loaded from the evaluation trial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbc8494675a1911",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T21:06:00.966875Z",
     "start_time": "2025-04-20T21:06:00.963366Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrandom\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.agents import create_openai_functions_agent\n",
    "from langchain.agents import AgentExecutor\n",
    "import re  # For regex-based extraction of scores and explanations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1879c22a3ce11797",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3bbf47b3b08f2c5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T21:06:00.996442Z",
     "start_time": "2025-04-20T21:06:00.994209Z"
    }
   },
   "outputs": [],
   "source": [
    "# data_path = 'data.csv'\n",
    "# df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a2edf8cb621711c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T21:06:01.022165Z",
     "start_time": "2025-04-20T21:06:01.017174Z"
    }
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"repository\": [\"project_1\", \"project_1\", \"project_1\", \"project_2\", \"project_2\", \"project_2\", \"project_3\", \"project_3\", \"project_3\"],\n",
    "    \"question\": [\n",
    "        \"How well does the code adhere to SOLID principles?\",\n",
    "        \"Are there any security vulnerabilities in the code?\",\n",
    "        \"How comprehensive are the unit tests?\",\n",
    "        \"Does the code follow consistent naming conventions?\",\n",
    "        \"Is the code well-documented?\",\n",
    "        \"How efficient is the algorithm?\",\n",
    "        \"Does the code follow consistent naming conventions?\",\n",
    "        \"How easy is it to extend or modify the code?\",\n",
    "        \"How well is the code optimized for performance?\"\n",
    "    ],\n",
    "    \"response\": [\n",
    "        [\"The code mostly follows SOLID principles, but there are some areas that could be improved, particularly with respect to single responsibility.\",\n",
    "         \"SOLID principles are well maintained, but there could be a better separation of concerns.\",\n",
    "         \"The code does a decent job with SOLID principles, but has minor violations in dependency inversion.\"],\n",
    "\n",
    "        [\"There are a few minor security issues, such as the use of outdated libraries.\",\n",
    "         \"The code has some potential security flaws related to hardcoded credentials and lack of input validation.\",\n",
    "         \"No obvious security issues, but some areas could benefit from more input sanitization.\"],\n",
    "\n",
    "        [\"Unit tests are mostly comprehensive, but there are some edge cases that haven't been covered.\",\n",
    "         \"The unit tests are thorough and cover most of the code, but a few areas, particularly error handling, are missing tests.\",\n",
    "         \"The tests are somewhat comprehensive, but certain critical paths are not tested.\"],\n",
    "\n",
    "        [\"The naming conventions are mostly consistent, with some occasional deviations.\",\n",
    "         \"The code follows naming conventions well, though there are some inconsistencies in variable names.\",\n",
    "         \"Naming conventions are adhered to in most places, but some methods and classes have unclear names.\"],\n",
    "\n",
    "        [\"The code is well-documented, with clear function-level docstrings and usage examples.\",\n",
    "         \"Documentation is decent, but some parts of the codebase lack sufficient comments and explanations.\",\n",
    "         \"The code is under-documented, and some complex parts of the system could use more detailed explanations.\"],\n",
    "\n",
    "        [\"The algorithm is efficient, but could be improved by optimizing the data structures used in certain parts of the code.\",\n",
    "         \"The algorithm works, but it could be more efficient, especially with large datasets.\",\n",
    "         \"The algorithm is relatively efficient but could benefit from better memory management and reduced complexity.\"],\n",
    "\n",
    "        [\"The naming conventions are consistently followed, making the codebase easy to navigate.\",\n",
    "         \"The naming conventions are good but could be slightly more descriptive in some areas.\",\n",
    "         \"There are a few inconsistencies in naming conventions, particularly with function names.\"],\n",
    "\n",
    "        [\"The code is highly modular and easy to extend, with clear separation of concerns.\",\n",
    "         \"The code is extendable, but some parts of the logic could be refactored to make it easier to add new features.\",\n",
    "         \"The code is somewhat extendable, but some tight coupling between components makes modifications challenging.\"],\n",
    "\n",
    "        [\"The code is optimized for performance and handles large datasets efficiently.\",\n",
    "         \"The performance is decent, but some areas of the code could benefit from further optimization, especially in terms of memory usage.\",\n",
    "         \"Performance is a concern in the current implementation, as there are some bottlenecks that could be optimized.\"]\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "703526ed6c71bb56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T21:06:01.073656Z",
     "start_time": "2025-04-20T21:06:01.069653Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the DataFrame\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408e2310",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb729723b209ec7",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b05d73ddd35fb9d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T21:07:38.970633Z",
     "start_time": "2025-04-20T21:07:38.964032Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to send query to Ollama using LangChain (no memory)\n",
    "def send_to_ollama(query):\n",
    "    # Initialize the ChatOllama instance (no memory involved)\n",
    "    llm = ChatOllama(\n",
    "        model=\"mistral\",  # Specify your model here\n",
    "        temperature=0.5\n",
    "    )\n",
    "\n",
    "    # Create a simple prompt template (pass the query as part of a mapping, not just a string)\n",
    "    prompt = PromptTemplate(input_variables=[\"query\"], template=query)\n",
    "\n",
    "    # Create a RunnableSequence chain\n",
    "    llm_chain = prompt | llm  # Use | operator to chain the prompt with the model\n",
    "\n",
    "    # Send query to Ollama and get the response\n",
    "    response = llm_chain.invoke({\"query\": query})  # Pass the query as a mapping\n",
    "\n",
    "    # Check if the response is an AIMessage object and get the text content\n",
    "    if hasattr(response, 'text'):\n",
    "        response_text = response.content  # Get the actual text content from the AIMessage object\n",
    "    else:\n",
    "        raise ValueError(\"Response does not contain text attribute.\")\n",
    "\n",
    "    # Initialize empty lists to store scores and explanations\n",
    "    scores = []\n",
    "    explanations = []\n",
    "\n",
    "    # Regular expression patterns for different parts of the response\n",
    "    response_num_pattern = re.compile(r\"Response\\s*(\\d+)\")  # Match 'Response X'\n",
    "    score_pattern = re.compile(r\"Score\\s*-\\s*(\\d+)\")  # Match 'Score - X'\n",
    "    explanation_pattern = re.compile(r\"Explanation.\\s*(.*)\")  # Match 'Explanation: ...'\n",
    "\n",
    "    # Variables to hold the extracted values\n",
    "    current_score = None\n",
    "    current_explanation = None\n",
    "\n",
    "    # Iterate through each line in the response text\n",
    "    for line in response_text.split(\"\\n\"):\n",
    "        # Extract response number\n",
    "        response_num_match = response_num_pattern.search(line)\n",
    "        if response_num_match:\n",
    "            response_num = response_num_match.group(1)\n",
    "\n",
    "        # Extract score\n",
    "        score_match = score_pattern.search(line)\n",
    "        if score_match:\n",
    "            current_score = int(score_match.group(1))  # Capture the score as an integer\n",
    "\n",
    "        # Extract explanation\n",
    "        explanation_match = explanation_pattern.search(line)\n",
    "        if explanation_match:\n",
    "            current_explanation = explanation_match.group(1).strip()  # Capture the explanation\n",
    "\n",
    "        # Once both score and explanation are found, store them\n",
    "        if current_score is not None and current_explanation is not None:\n",
    "            scores.append(current_score)\n",
    "            explanations.append(current_explanation)\n",
    "\n",
    "            # Reset for next response\n",
    "            current_score = None\n",
    "            current_explanation = None\n",
    "\n",
    "    return scores, explanations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "21444c1fa05a4386",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T21:33:26.868842Z",
     "start_time": "2025-04-20T21:33:26.861264Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_and_map_scores(flattened_data):\n",
    "    # Step 1: Prepare the query for evaluation, process each response individually\n",
    "    query_base = \"\"\"\n",
    "        You are an experienced evaluator assessing responses from developers to technical questions.\n",
    "\n",
    "        For each question, the responses from the developers have been provided. Evaluate how well each response answers the question on a scale of 1-10, where:\n",
    "        - 1 means the response is completely irrelevant or fails to answer the question.\n",
    "        - 10 means the response fully answers the question, providing a thorough, clear, and correct explanation.\n",
    "\n",
    "        For each response:\n",
    "        - Consider the relevance of the response to the question.\n",
    "        - Consider the completeness of the answer. Does the response cover all parts of the question?\n",
    "        - Consider the clarity of the response. Is it easy to understand and well-explained?\n",
    "\n",
    "        Return your evaluation in the following format:\n",
    "        Response X: Score - X Explanation: Explanation of why you gave this score.\n",
    "\n",
    "        Example Response:\n",
    "        Response 1: Score - 4 Explanation: While the response is relevant to the question it fails to provide contextual details.\n",
    "\n",
    "        Here is the question and response:\n",
    "    \"\"\"\n",
    "\n",
    "    evaluation_results = []  # To store the results (scores and explanations)\n",
    "\n",
    "    # Iterate over each row in the flattened data to send each response individually\n",
    "    for idx, row in flattened_data.iterrows():\n",
    "        repository = row['repository']\n",
    "        question = row['question']\n",
    "        response = row['response']\n",
    "        original_index = row['original_index']\n",
    "        response_order = row['response_order']\n",
    "\n",
    "        # Construct the query for this specific response\n",
    "        query = query_base + f\"\\t{question}: {response}\\n\"\n",
    "        query += \"\\t\\t---\\n\\t\\tPlease provide a score (1-10) and an explanation for this response.\"\n",
    "\n",
    "        # Step 2: Send the query to Ollama (or any other evaluator)\n",
    "        ollama_scores, ollama_explanations = send_to_ollama(query)  # Make sure to define this function to send and get responses\n",
    "\n",
    "        # Step 3: Store the score and explanation for this response\n",
    "        evaluation_results.append({\n",
    "            'repository': repository,\n",
    "            'question': question,\n",
    "            'response': response,\n",
    "            'original_index': original_index,\n",
    "            'response_order': response_order,\n",
    "            'score': ollama_scores[0],  # Assuming ollama_scores is a list of length 1 for each query\n",
    "            'explanation': ollama_explanations[0]  # Assuming ollama_explanations is a list of length 1\n",
    "        })\n",
    "\n",
    "    # Step 4: Convert the results to a DataFrame\n",
    "    evaluation_df = pd.DataFrame(evaluation_results)\n",
    "    return evaluation_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "d06b7c20398c21c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T21:37:52.204670Z",
     "start_time": "2025-04-20T21:37:52.188281Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 2: Add an 'original_index' column and 'response_order' to capture the order of responses\n",
    "df['original_index'] = df.index\n",
    "df['response_order'] = df['response'].apply(lambda x: list(range(len(x))))\n",
    "\n",
    "# Step 3: Explode the data and keep track of the response order\n",
    "df_exploded = df.explode('response', ignore_index=True)\n",
    "df_exploded['response_order'] = df_exploded.groupby(['repository', 'question']).cumcount()\n",
    "\n",
    "# Step 4: Shuffle the exploded data (responses) but retain 'response_order' for tracking\n",
    "df_shuffled = df_exploded.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "6cb6f1df03aff4f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T21:40:19.253258Z",
     "start_time": "2025-04-20T21:37:55.939310Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluation_df = process_and_map_scores(df_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "bcbc8869388ab198",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T21:45:46.608684Z",
     "start_time": "2025-04-20T21:45:46.602714Z"
    }
   },
   "outputs": [],
   "source": [
    "df_restored = evaluation_df.sort_values(by=['original_index', 'response_order']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "39204574cbe75587",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T21:46:30.245056Z",
     "start_time": "2025-04-20T21:45:57.895565Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   repository                                           question  \\\n",
      "0   project_1  How well does the code adhere to SOLID princip...   \n",
      "1   project_1  How well does the code adhere to SOLID princip...   \n",
      "2   project_1  How well does the code adhere to SOLID princip...   \n",
      "3   project_1  Are there any security vulnerabilities in the ...   \n",
      "4   project_1  Are there any security vulnerabilities in the ...   \n",
      "5   project_1  Are there any security vulnerabilities in the ...   \n",
      "6   project_1              How comprehensive are the unit tests?   \n",
      "7   project_1              How comprehensive are the unit tests?   \n",
      "8   project_1              How comprehensive are the unit tests?   \n",
      "9   project_2  Does the code follow consistent naming convent...   \n",
      "10  project_2  Does the code follow consistent naming convent...   \n",
      "11  project_2  Does the code follow consistent naming convent...   \n",
      "12  project_2                       Is the code well-documented?   \n",
      "13  project_2                       Is the code well-documented?   \n",
      "14  project_2                       Is the code well-documented?   \n",
      "15  project_2                    How efficient is the algorithm?   \n",
      "16  project_2                    How efficient is the algorithm?   \n",
      "17  project_2                    How efficient is the algorithm?   \n",
      "18  project_3  Does the code follow consistent naming convent...   \n",
      "19  project_3  Does the code follow consistent naming convent...   \n",
      "20  project_3  Does the code follow consistent naming convent...   \n",
      "21  project_3       How easy is it to extend or modify the code?   \n",
      "22  project_3       How easy is it to extend or modify the code?   \n",
      "23  project_3       How easy is it to extend or modify the code?   \n",
      "24  project_3    How well is the code optimized for performance?   \n",
      "25  project_3    How well is the code optimized for performance?   \n",
      "26  project_3    How well is the code optimized for performance?   \n",
      "\n",
      "                                             response  original_index  \\\n",
      "0   The code mostly follows SOLID principles, but ...               0   \n",
      "1   SOLID principles are well maintained, but ther...               0   \n",
      "2   The code does a decent job with SOLID principl...               0   \n",
      "3   There are a few minor security issues, such as...               1   \n",
      "4   The code has some potential security flaws rel...               1   \n",
      "5   No obvious security issues, but some areas cou...               1   \n",
      "6   Unit tests are mostly comprehensive, but there...               2   \n",
      "7   The unit tests are thorough and cover most of ...               2   \n",
      "8   The tests are somewhat comprehensive, but cert...               2   \n",
      "9   The naming conventions are mostly consistent, ...               3   \n",
      "10  The code follows naming conventions well, thou...               3   \n",
      "11  Naming conventions are adhered to in most plac...               3   \n",
      "12  The code is well-documented, with clear functi...               4   \n",
      "13  Documentation is decent, but some parts of the...               4   \n",
      "14  The code is under-documented, and some complex...               4   \n",
      "15  The algorithm is efficient, but could be impro...               5   \n",
      "16  The algorithm works, but it could be more effi...               5   \n",
      "17  The algorithm is relatively efficient but coul...               5   \n",
      "18  The naming conventions are consistently follow...               6   \n",
      "19  The naming conventions are good but could be s...               6   \n",
      "20  There are a few inconsistencies in naming conv...               6   \n",
      "21  The code is highly modular and easy to extend,...               7   \n",
      "22  The code is extendable, but some parts of the ...               7   \n",
      "23  The code is somewhat extendable, but some tigh...               7   \n",
      "24  The code is optimized for performance and hand...               8   \n",
      "25  The performance is decent, but some areas of t...               8   \n",
      "26  Performance is a concern in the current implem...               8   \n",
      "\n",
      "    response_order  score                                        explanation  \n",
      "0                0      7  The response is relevant to the question as it...  \n",
      "1                1      7  The response is relevant to the question as it...  \n",
      "2                2      7  The response is relevant to the question as it...  \n",
      "3                0      7  The response is relevant to the question, but ...  \n",
      "4                1      8  While the response is mostly relevant and answ...  \n",
      "5                2      7  The response is relevant to the question as it...  \n",
      "6                0      7  The response is relevant to the question and p...  \n",
      "7                1      8  The response is relevant to the question, prov...  \n",
      "8                2      6  While the response is relevant to the question...  \n",
      "9                0      8  The response is relevant to the question as it...  \n",
      "10               1      8  This response is mostly relevant to the questi...  \n",
      "11               2      6  The response is somewhat relevant as it acknow...  \n",
      "12               0     10  This response fully answers the question by co...  \n",
      "13               1      7  The response is somewhat relevant as it acknow...  \n",
      "14               2      7  While the response is relevant to the question...  \n",
      "15               0      7  The response is relevant to the question, as i...  \n",
      "16               1      8  The response is relevant to the question as it...  \n",
      "17               2      7  The response does address the question by prov...  \n",
      "18               0      8  While the response is relevant to the question...  \n",
      "19               1      8  The response is relevant to the question as it...  \n",
      "20               2      6  The response is somewhat relevant to the quest...  \n",
      "21               0      8  While the response is relevant and provides a ...  \n",
      "22               1      7  The response is relevant to the question and p...  \n",
      "23               2      7  This response is moderately relevant to the qu...  \n",
      "24               0      8  The response is relevant to the question, as i...  \n",
      "25               1      7  This response is somewhat relevant to the ques...  \n",
      "26               2      7  The response is relevant to the question and p...  \n"
     ]
    }
   ],
   "source": [
    "# Display the results\n",
    "print(df_restored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a976ead2bd3ca37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code-explore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
